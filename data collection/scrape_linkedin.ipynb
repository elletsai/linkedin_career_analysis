{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape username from LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:15:02.690621Z",
     "start_time": "2020-04-12T20:15:02.686627Z"
    }
   },
   "outputs": [],
   "source": [
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm #check for the scarping progress\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from scrape_linkedin import ProfileScraper\n",
    "from selenium.webdriver.common.by import By\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T06:36:02.876233Z",
     "start_time": "2020-04-10T06:36:02.869853Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_name():\n",
    "    # get username\n",
    "    name_list = []\n",
    "    #driver.page_source is a entire page's HTML\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser') # this step get all the static page info, BeautifulSoup is a object\n",
    "    \n",
    "    for a in soup.find(class_=\"search-results__list list-style-none \").find_all('a', href=True):\n",
    "        name = a['href'].split('/')[-2]  #a['href'] : /in/anastasisbele/\n",
    "        if a['href'].split('/')[1] == 'in' and name not in name_list:\n",
    "            name_list.append(name)\n",
    "    batch(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T06:34:44.262940Z",
     "start_time": "2020-04-10T06:34:44.258446Z"
    }
   },
   "outputs": [],
   "source": [
    "#not need for now\n",
    "# save while scraping\n",
    "import pickle\n",
    "def batch(name):\n",
    "    with open('name.pkl', 'ab') as fp: \n",
    "        #with open(file_name, mode), use with makes sure the file is closed after the session, 'a' for append, 'b' for binary mode\n",
    "        pickle.dump(name, fp) #pickle.dump(obj, file), write object into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T06:37:22.828250Z",
     "start_time": "2020-04-10T06:36:06.903321Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.23s/it]\n"
     ]
    }
   ],
   "source": [
    "#dynamic scraping\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.linkedin.com/search/results/people/?keywords=data%20analyst&origin=SWITCH_SEARCH_VERTICAL\"\n",
    "driver.get(url)\n",
    "\n",
    "#login linkedin\n",
    "driver.find_element_by_css_selector('body > div > main > p > a').click()\n",
    "time.sleep(0.5)\n",
    "driver.find_element_by_css_selector('input#username').send_keys('zz629@cornell.edu') # your email\n",
    "driver.find_element_by_css_selector('input#password').send_keys('Zx16m73yl') # your password for linkedin\n",
    "driver.find_element_by_css_selector('#app__container > main > div:nth-child(2) > form > div.login__form_action_container > button').click()\n",
    "\n",
    "for i in tqdm(range(1)): # how many pages do we want to scrape\n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # scroll down and wait website to load\n",
    "    time.sleep(10)\n",
    "    \n",
    "    name_list = []\n",
    "    get_name()  # get name by using Beautiful soup\n",
    "    time.sleep(5)\n",
    "\n",
    "    # click on next bottom to go to next page\n",
    "    xpath = '//button[contains(@aria-label,\"Next\")]'\n",
    "    driver.find_element(By.XPATH, xpath).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get name id from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:15:05.470182Z",
     "start_time": "2020-04-12T20:15:05.463539Z"
    }
   },
   "outputs": [],
   "source": [
    "#not need for now\n",
    "# open name.pkl file\n",
    "name = []\n",
    "with open('name.pkl', 'rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            name.append(pickle.load(fr)) \n",
    "            #pickle.load(fr): pickle.load(file)\n",
    "            #read the object from the file and return the reconstituted object hierarchy specified therein\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "from itertools import chain\n",
    "name_list = list(chain.from_iterable(name)) #chain thins in different arrays together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use write csv to save by rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:19:29.020335Z",
     "start_time": "2020-04-12T20:19:01.748434Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "LI_AT = 'AQEDASqt89oFOYnbAAABcB2vVKkAAAFxeby1ZU0APL5S3jkundKs8NMku9JqGfod9KTZSvenbZTjqIv5LLffYlAmjiNpGiHad6WIlwDbX2-TfVS56hWIcBUB4wW0CZ2y7uIM_BBcikJXup38mpvfiUwA'\n",
    "# elle's\n",
    "# LI_AT = 'AQEDASninKgFCQENAAABcWJ8RbkAAAFxhojJuU0ARn_JC06BmSzB8515Xro1AqXinajR4vaRFt5_RFnE1Yc9_6F1VWCAS9BjkPnfqHkirT9j3Mjdb_U0JpJ8EDgfItrqRsndIjAMxyTc0LqheG64WKPJ'\n",
    "\n",
    "# users = ['yuchen-miranda-zhao', 'jamie-yu-3348434b', 'rongxinz', 'ziyi-li-333a21168'] \n",
    "\n",
    "with open('experience.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for name in name_list:\n",
    "        with ProfileScraper(cookie = LI_AT) as scraper:\n",
    "            profile = scraper.scrape(user = name)\n",
    "        for job in profile.experiences['jobs']:\n",
    "            writer.writerow([name, job['title'], job['company'], job['date_range'], job['location'], job['description']])\n",
    "            #print(name, job['title'], job['company'], job['date range'], job['location'], job['description'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use following to save simultaneously in two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:44:55.215548Z",
     "start_time": "2020-04-12T20:44:55.211188Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def info_batch(info):\n",
    "    with open('info.pkl', 'ab') as fp: \n",
    "        pickle.dump(info, fp) \n",
    "    \n",
    "def exp_batch(exp):\n",
    "    with open('exp.pkl', 'ab') as fp:\n",
    "        pickle.dump(exp, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:45:59.543710Z",
     "start_time": "2020-04-12T20:45:10.295126Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in users:\n",
    "    with ProfileScraper(cookie = LI_AT) as scraper:\n",
    "        profile = scraper.scrape(user = name)\n",
    "        \n",
    "    # save to info file        \n",
    "    summary = profile.personal_info['summary']\n",
    "    skill = [profile.skills[i]['name'] for i in range(len(profile.skills))]\n",
    "    certi = profile.accomplishments['certifications']\n",
    "    course = profile.accomplishments['courses']\n",
    "\n",
    "    info_batch(tuple([name, summary, skill, course, certi]))\n",
    "    \n",
    "    # save to exp file\n",
    "    for job in profile.experiences['jobs']:\n",
    "        exp_batch(tuple([name, job['title'], job['company'], job['date_range'], job['location'], job['description']]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pickle to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:46:03.862813Z",
     "start_time": "2020-04-12T20:46:03.840397Z"
    }
   },
   "outputs": [],
   "source": [
    "# call info pickle file\n",
    "colnames = ['name', 'summary', 'skill', 'course', 'certi']\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "info = []\n",
    "with open('info.pkl', 'rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            info.append(pickle.load(fr))\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "info_df = pd.DataFrame(info, columns=colnames)\n",
    "info_df\n",
    "# info_df.to_csv('info.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:46:26.151364Z",
     "start_time": "2020-04-12T20:46:26.133644Z"
    }
   },
   "outputs": [],
   "source": [
    "# call exp pickle file\n",
    "colnames = ['name', 'title', 'company', 'date_range', 'location', 'description']\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "exp = []\n",
    "with open('exp.pkl', 'rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            exp.append(pickle.load(fr))\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "exp_df = pd.DataFrame(exp, columns=colnames)\n",
    "exp_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
